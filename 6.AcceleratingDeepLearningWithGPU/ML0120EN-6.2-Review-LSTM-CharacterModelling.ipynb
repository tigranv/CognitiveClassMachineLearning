{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "\n",
    "# <center> Text generation using RNN/LSTM (Character-level)</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 3><strong>In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network</strong></font>\n",
    "<br>    \n",
    "- <a href=\"#intro\">Introduction</a>\n",
    "<br>\n",
    "- <p><a href=\"#arch\">Architectures</a></p>\n",
    "    - <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a>\n",
    "\n",
    "- <p><a href=\"#build\">Building a LSTM with TensorFlow</a></p>\n",
    "</div>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the requiered libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "The following cell is a class that help to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our dataset, with real parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 60  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 1115393 / 1115393"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "url = 'https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt'\n",
    "filename = wget.download(url, out=\"input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the input file, and print a part of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Sample text---------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print(\"-------------Sample text---------------\")\n",
    "    print (read_data[0:500])\n",
    "    print(\"---------------------------------------\")\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [19  4 14 ... 14  9 20]\n",
      " [ 8 20 10 ...  8 10 18]\n",
      " ...\n",
      " [21  2  0 ...  0 21  0]\n",
      " [ 9  7  7 ...  0  2  3]\n",
      " [ 3  7  0 ...  5  9 23]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =60, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ...,\n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture\n",
    "Each LSTM cell has 5 parts:\n",
    "1. Input\n",
    "2. prv_state\n",
    "3. prv_output\n",
    "4. new_state\n",
    "5. new_output\n",
    "\n",
    "\n",
    "- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n",
    "- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n",
    "- An LSTM keeps two pieces of information as it propagates through time: \n",
    "    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
    "    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
    "\n",
    "\n",
    "#### num_layers = 2 \n",
    "- number of layers in the RNN, is defined by num_layers\n",
    "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stacked RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__BasicRNNCell__ is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) \n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1497027 ,  0.12158741, -0.01893631, ...,  0.17223085,\n",
       "        -0.05599986, -0.15722314],\n",
       "       [ 0.16438274, -0.03280082,  0.04960006, ...,  0.04070774,\n",
       "         0.02506849, -0.02037479],\n",
       "       [-0.04120316,  0.08913623, -0.1683996 , ..., -0.00584108,\n",
       "         0.02681051,  0.10248314],\n",
       "       ...,\n",
       "       [-0.01828451, -0.00664853, -0.058921  , ..., -0.03101338,\n",
       "        -0.02481088,  0.08020641],\n",
       "       [-0.13334885,  0.00496854,  0.0793155 , ..., -0.17576866,\n",
       "        -0.10947511,  0.13538076],\n",
       "       [ 0.06756967,  0.10279985, -0.08793063, ...,  0.12172104,\n",
       "         0.15053512, -0.03866233]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.09479998,  0.1351005 , -0.1561817 , ...,  0.03654855,\n",
       "         0.12002201,  0.00681794],\n",
       "       [-0.0519297 ,  0.03958312,  0.0079221 , ..., -0.06718533,\n",
       "        -0.04112677, -0.00938937],\n",
       "       [ 0.1523598 , -0.13082966, -0.09595912, ...,  0.10043801,\n",
       "        -0.07540908, -0.15614545],\n",
       "       ...,\n",
       "       [ 0.16438274, -0.03280082,  0.04960006, ...,  0.04070774,\n",
       "         0.02506849, -0.02037479],\n",
       "       [ 0.00576486, -0.10755866,  0.15060727, ...,  0.00477269,\n",
       "         0.07039295,  0.02554522],\n",
       "       [ 0.1523598 , -0.13082966, -0.09595912, ...,  0.10043801,\n",
       "        -0.07540908, -0.15614545]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding a batch of 50 sequence to a RNN:\n",
    "\n",
    "The feeding process for iputs is as following:\n",
    "\n",
    "- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n",
    "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
    "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
    "\n",
    "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09479998,  0.1351005 , -0.1561817 , ...,  0.03654855,\n",
       "         0.12002201,  0.00681794],\n",
       "       [ 0.06112112,  0.14853464, -0.02428205, ..., -0.04666328,\n",
       "         0.04466999,  0.12623833],\n",
       "       [-0.1702205 ,  0.06078452, -0.15013573, ..., -0.10139243,\n",
       "        -0.10528401,  0.09119491],\n",
       "       ...,\n",
       "       [ 0.13629498, -0.17182527,  0.0634447 , ..., -0.09407214,\n",
       "         0.07504208, -0.1633999 ],\n",
       "       [-0.0519297 ,  0.03958312,  0.0079221 , ..., -0.06718533,\n",
       "        -0.04112677, -0.00938937],\n",
       "       [-0.00210688, -0.08544768, -0.17344838, ...,  0.03801389,\n",
       "        -0.09992099,  0.16979529]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07154685,  0.02927342, -0.05966241, ...,  0.10146965,\n",
       "         0.03951957,  0.02573378],\n",
       "       [ 0.16064721,  0.018649  ,  0.03379982, ...,  0.08874944,\n",
       "        -0.0769235 , -0.03618045],\n",
       "       [ 0.02640805, -0.013596  , -0.05861655, ..., -0.08360599,\n",
       "         0.07201909,  0.04725482],\n",
       "       ...,\n",
       "       [-0.01737448,  0.02370217,  0.04505913, ...,  0.02039089,\n",
       "         0.01984148, -0.0442081 ],\n",
       "       [-0.08745293, -0.0440765 , -0.00121199, ...,  0.06485443,\n",
       "         0.03385902, -0.04940572],\n",
       "       [-0.01196583, -0.00928249,  0.05373045, ...,  0.04929352,\n",
       "         0.04905186,  0.03113439]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01393263, 0.01688068, 0.01467786, ..., 0.01782871, 0.01687207,\n",
       "        0.014497  ],\n",
       "       [0.01405734, 0.0179311 , 0.01480261, ..., 0.01648588, 0.01767599,\n",
       "        0.01435904],\n",
       "       [0.01395087, 0.0143767 , 0.01789393, ..., 0.01735869, 0.01907594,\n",
       "        0.01394723],\n",
       "       ...,\n",
       "       [0.01591193, 0.01361613, 0.01409819, ..., 0.01530496, 0.0183557 ,\n",
       "        0.01563397],\n",
       "       [0.01752298, 0.01790323, 0.01163498, ..., 0.01568443, 0.01460208,\n",
       "        0.01361894],\n",
       "       [0.01652607, 0.0107215 , 0.01138638, ..., 0.01436388, 0.01597863,\n",
       "        0.01604275]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            #print(\">> sample mode:\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "            inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            #inputs = tf.split(em, seq_length, 1)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the LSTM object\n",
    "Now we create a LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train usinng LSTMModel class\n",
    "We can train our model through feeding batches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>*You can run this cell if you REALLY have time to wait, or you are running it using PowerAI </strong></font>\n",
    "\n",
    "\n",
    "What is PowerAI?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/DX0108EN-PowerAI).\n",
    "\n",
    "\n",
    "__Notice:__ If you are running this notebook on PowerAI, it will automatically run on a GPU, otherwise it will use a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/46375 (epoch 0), train_loss = 1.925, time/batch = 0.046\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The lory lay worary dabe.\n",
      "\n",
      "feriget dears,\n",
      "Rit may, it deeT-w an aust say\n",
      "To suce be feace.\n",
      "\n",
      "DUKI VINCENBESTI:\n",
      "I why leart, on heve the not, for Dome\n",
      "If he\n",
      "----------------------------------\n",
      "741/46375 (epoch 1), train_loss = 1.755, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The owe amary:\n",
      "Then olf her frow one, thou hath wagh rageland must,\n",
      "Waspings up woll, he wingoes thum adver hidsed gromen,\n",
      "There?\n",
      "\n",
      "QLANDI:\n",
      "Yee the mine wh\n",
      "----------------------------------\n",
      "1112/46375 (epoch 2), train_loss = 1.673, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The uslave-bubfer bittle\n",
      "This guch of their caysed firely and they Down?\n",
      "O\n",
      "under king not feech\n",
      "I'll last it think lasdol\n",
      "Wy prothe\n",
      "As them the face your \n",
      "----------------------------------\n",
      "1483/46375 (epoch 3), train_loss = 1.625, time/batch = 0.028\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The own, on whow I dreens all to hangica seece, on againet teriknt\n",
      "My prowert Haster deder'd.\n",
      "Maged with thy devel I\n",
      "\n",
      "Nor Cary humpalle; alife trues.\n",
      "\n",
      "BIO\n",
      "----------------------------------\n",
      "1854/46375 (epoch 4), train_loss = 1.595, time/batch = 0.046\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The more; I went on wime extrman!\n",
      "He graviaging, but done.\n",
      "What's the him, his gove, indue? Whither, we us that of our apoe so,\n",
      "Why, threw on it, Ciring I\n",
      "----------------------------------\n",
      "2225/46375 (epoch 5), train_loss = 1.577, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The been thou cannot them would plunts let uctenome, from Have comeern sugh our brother,\n",
      "I do your our heartion\n",
      "Yound.\n",
      "\n",
      "BUCKINGCUS:\n",
      "Novided him asmel but \n",
      "----------------------------------\n",
      "2596/46375 (epoch 6), train_loss = 1.566, time/batch = 0.041\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The meriven hands,\n",
      "Of all the unsplace; nave scrowncire grounds mistard: and ging he same I\n",
      "bear yes.\n",
      "Sir by and fruthe. O, are that unourt;\n",
      "And thou liet\n",
      "----------------------------------\n",
      "2967/46375 (epoch 7), train_loss = 1.558, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The should do the note; them make his natesty.\n",
      "\n",
      "GONZANO!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "All.\n",
      "\n",
      "LORD EINNE:\n",
      "Come would better Parmings far joyful time mind,\n",
      "custime to fo\n",
      "----------------------------------\n",
      "3338/46375 (epoch 8), train_loss = 1.552, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The madness it,\n",
      "A diser's feel us a leave to my satelves evan lenr thee, lucent, far on a more monstiem\n",
      "as eas\n",
      "Alive, and I come touchion-happose heart yo\n",
      "----------------------------------\n",
      "3709/46375 (epoch 9), train_loss = 1.547, time/batch = 0.038\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The firrent;\n",
      "And your soul entreat\n",
      "As let me by proud.\n",
      "\n",
      "MORTAUF:\n",
      "Why, I'll fraited he stere the govingble of my lare that thou dature come on me aclerge d\n",
      "----------------------------------\n",
      "4080/46375 (epoch 10), train_loss = 1.541, time/batch = 0.037\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The moon;\n",
      "And so caumes:\n",
      "Welch awaying a peat that him; I gord I am catchnor country; and be beat\n",
      "you:\n",
      "There's from that, issuaring part the word arm in t\n",
      "----------------------------------\n",
      "4451/46375 (epoch 11), train_loss = 1.536, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The have with beast, and in man, indeed, for infatis,\n",
      "Verivy prevenged, sicenty\n",
      "Shame; sir?\n",
      "\n",
      "EDWARD:\n",
      "Gentle 'solaze.\n",
      "No, and sweash every fair-tiaity been\n",
      "----------------------------------\n",
      "4822/46375 (epoch 12), train_loss = 1.531, time/batch = 0.039\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The vidiness' citizen:\n",
      "Sir, 'tis to\n",
      "The worse:\n",
      "If unform, foe,\n",
      "'Tis beatter tire.\n",
      "\n",
      "SEcre:\n",
      "Dadal Jesestingle to this his monnate own to the brool,\n",
      "Go ender\n",
      "----------------------------------\n",
      "5193/46375 (epoch 13), train_loss = 1.526, time/batch = 0.037\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The ruble.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Now, not of him!\n",
      "that take the penabory them thee 'twell.\n",
      "\n",
      "LEON:\n",
      "How thou disuse me.\n",
      "\n",
      "PETRUCHIO:\n",
      "Agat her scape,\n",
      "As bride of \n",
      "----------------------------------\n",
      "5564/46375 (epoch 14), train_loss = 1.521, time/batch = 0.039\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The they shall bear stand that Edwentle, it make for liege,\n",
      "Sometter at misceman from the heaw, thou, sir, fenceous not.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I theme rost?\n",
      "\n",
      "N\n",
      "----------------------------------\n",
      "5935/46375 (epoch 15), train_loss = 1.516, time/batch = 0.039\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wigniful house.\n",
      "so dispire me can of daughter o' that all, send my mutis kit shore your marrieved going, friends or doth sow of qualt, I hear I hear t\n",
      "----------------------------------\n",
      "6306/46375 (epoch 16), train_loss = 1.511, time/batch = 0.041\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The fit ever friend whic My aughtinghan first my word thir't: sire with the entents\n",
      "Train what I know, my queen to be burn:\n",
      "And have to tempose dreams the\n",
      "----------------------------------\n",
      "6677/46375 (epoch 17), train_loss = 1.506, time/batch = 0.040\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The oft our vecers!\n",
      "Let thee is the lacks, nor make differ'd thou horrel-pritpher is; for me;\n",
      "Your better, I with jount, and seems the truth for comport! \n",
      "----------------------------------\n",
      "7048/46375 (epoch 18), train_loss = 1.502, time/batch = 0.037\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wrong's houch is\n",
      "thy sin great this concent his lo,\n",
      "Eres suffer'd. JLAust:\n",
      "That the enter mine.\n",
      "\n",
      "KING RICHARD III:\n",
      "Significe.\n",
      "\n",
      "Nurse:\n",
      "'Sich I stay, yo\n",
      "----------------------------------\n",
      "7419/46375 (epoch 19), train_loss = 1.497, time/batch = 0.038\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The mightind bid righering prainit, is for there it, the sun: these serve,\n",
      "And came this fiers\n",
      "To sure bloody grave is acting more crumished offins my nev\n",
      "----------------------------------\n",
      "7790/46375 (epoch 20), train_loss = 1.493, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The world takes,\n",
      "And fence is his kingness the offence?\n",
      "\n",
      "First Murderer:\n",
      "Sweet do o' with\n",
      "startion are like the shads,\n",
      "My foul hap war, my true?\n",
      "\n",
      "CAPULET:\n",
      "----------------------------------\n",
      "8161/46375 (epoch 21), train_loss = 1.489, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The acceal forth deep as haiten'd and Johbelleof back king, it is so;\n",
      "For you did that are, We livin to--birther,\n",
      "And is no more follower awhice that a ki\n",
      "----------------------------------\n",
      "8532/46375 (epoch 22), train_loss = 1.486, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The madlifolctor, tell the washs to sunsing far us; what read him!\n",
      "\n",
      "Servant: for what murderen bunter alfilury, consues false.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "'Tis be a\n",
      "----------------------------------\n",
      "8903/46375 (epoch 23), train_loss = 1.483, time/batch = 0.044\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The once,\n",
      "And we have blood shall be war most convarrous uniurt to downy fashile.\n",
      "\n",
      "Second Murdee, nor stin such for true breath\n",
      "Hath noign\n",
      "constrain, old \n",
      "----------------------------------\n",
      "9274/46375 (epoch 24), train_loss = 1.480, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The confess their-spear.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Sir, Warwict a fights, good more with themselve,\n",
      "If I played Willine; for no haid!\n",
      "I moulds, I do be heat, broth\n",
      "----------------------------------\n",
      "9645/46375 (epoch 25), train_loss = 1.478, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The mawf: sir. Fellow unrease thee as worn their suit will came, let's deed to the tistresses dances and come too date of you.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Which we f\n",
      "----------------------------------\n",
      "10016/46375 (epoch 26), train_loss = 1.476, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The very majesty,\n",
      "With what eparable once and my such part come!--but hnow, not i' the comery bear cause take much with my man.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Faith?\n",
      "\n",
      "\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10387/46375 (epoch 27), train_loss = 1.473, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The right to had way?\n",
      "\n",
      "JULIET:\n",
      "Come his.\n",
      "\n",
      "CORIOLANUS:\n",
      "This delivenest\n",
      "their court the fool soul!\n",
      "\n",
      "Servant:\n",
      "Naums.\n",
      "\n",
      "JULIET:\n",
      "No, let's descul, deams oft bea\n",
      "----------------------------------\n",
      "10758/46375 (epoch 28), train_loss = 1.471, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The field;\n",
      "Since that's you foul you\n",
      "nothing, to still? And you are content not other unpide pine:\n",
      "But not yet consenting Lucio; 'twis might diving as we,\n",
      "----------------------------------\n",
      "11129/46375 (epoch 29), train_loss = 1.469, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The fire the protest, have scriked not fight of Romeo, for entrechings, there, there I begins against the\n",
      "artupe, man's she danterness I think the might t\n",
      "----------------------------------\n",
      "11500/46375 (epoch 30), train_loss = 1.467, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The troub!\n",
      "\n",
      "FLORIZEL:\n",
      "As't, I beseech the parting sweet marry, there tignior blame my mighty enemy, licks the son's eyes.\n",
      "Ah? till not her angles disproce\n",
      "----------------------------------\n",
      "11871/46375 (epoch 31), train_loss = 1.465, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Camelade so grant yet a charged; being save his choss'd of my man.\n",
      "\n",
      "HORTENSIO:\n",
      "By harbid;\n",
      "Luest me to-morrow, as comes sents slone. \n",
      "KING EDWARD IV:\n",
      "H\n",
      "----------------------------------\n",
      "12242/46375 (epoch 32), train_loss = 1.463, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The confess there?\n",
      "\n",
      "ARCHALINA:\n",
      "Yourgely,\n",
      "Being should the berrotful insuls his night him\n",
      "Which:\n",
      "But good lords,\n",
      "I'll make about at help it biest myselfy, \n",
      "----------------------------------\n",
      "12613/46375 (epoch 33), train_loss = 1.461, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The king gome from heart.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Hall wind me command:\n",
      "My lord.\n",
      "\n",
      "THOMAS:\n",
      "Who afford stay us: my looks, will not rogged with one to the stone:\n",
      "----------------------------------\n",
      "12984/46375 (epoch 34), train_loss = 1.459, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The come one schard\n",
      "The giet them\n",
      "The prap to known saw him by threefel'd me herself; and the lead a khow-hoar? you say your king your hell\n",
      "A gird drinkin\n",
      "----------------------------------\n",
      "13355/46375 (epoch 35), train_loss = 1.457, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The king sleep,\n",
      "Have it?\n",
      "\n",
      "BENVOLIO:\n",
      "Upon a hand the\n",
      "slove a bastier, sir, have not before him not would King Lord Harsh! O, not altern hard! my knother to\n",
      "----------------------------------\n",
      "13726/46375 (epoch 36), train_loss = 1.455, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The leave on the predega ort, crawly.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why, you have now fair to your frether endire I'll make we me, who lost then mide much lord, the valian\n",
      "----------------------------------\n",
      "14097/46375 (epoch 37), train_loss = 1.454, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The come from you'll, gave my libe.\n",
      "\n",
      "Clown:\n",
      "Ay, untraings our night, York are Capsillenind.\n",
      "\n",
      "MAMILLIUS:\n",
      "Ay! fare for thus best mine goods uncle against th\n",
      "----------------------------------\n",
      "14468/46375 (epoch 38), train_loss = 1.452, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The face,\n",
      "And alcase have make us herwifil killenty to he within of the man us but again, but whether to be fooly as the woman, and shrubs: here you are y\n",
      "----------------------------------\n",
      "14839/46375 (epoch 39), train_loss = 1.451, time/batch = 0.031\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The unfold, then did dyed his corse more spent what is to curse driblet,\n",
      "Not our blood; his lean I fearing cloud me which the ballignish all\n",
      "The time it, \n",
      "----------------------------------\n",
      "15210/46375 (epoch 40), train_loss = 1.449, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The faithful.\n",
      "I behild thee he cast thy eat is carver Capites?\n",
      "\n",
      "ABROMERES\n",
      "On:\n",
      "Must like resolved seen the\n",
      "praise avocues them my friend; as that may, do I\n",
      "----------------------------------\n",
      "15581/46375 (epoch 41), train_loss = 1.448, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The tribune instruction after doth chephering at thy wife\n",
      "In great enjoy are he aptience's love you without at lamenticate do though it a peace, sir.\n",
      "\n",
      "DUK\n",
      "----------------------------------\n",
      "15952/46375 (epoch 42), train_loss = 1.446, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The way unstand in the\n",
      "hours accuse or but it the daughter, I reways such's tons, it worn she mannet it ho?\n",
      "\n",
      "Aginds I cannot please in acces: like you in \n",
      "----------------------------------\n",
      "16323/46375 (epoch 43), train_loss = 1.445, time/batch = 0.034\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The of me pronoers, my son's born it! here the names father,\n",
      "Wich good lord.\n",
      "\n",
      "Second Citizen:\n",
      "Richard any onemies;\n",
      "And whither, do brencours, mighty plaga\n",
      "----------------------------------\n",
      "16694/46375 (epoch 44), train_loss = 1.444, time/batch = 0.034\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Duke him. With lady's saw humoor:\n",
      "The combate no wife loved now by practiers, budster! Braded that all upon;\n",
      "Stand some maje.\n",
      "And Montain unto his\n",
      "cla\n",
      "----------------------------------\n",
      "17065/46375 (epoch 45), train_loss = 1.442, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The body is luce us to Rome, all the duke, in the happy, is not dead royal all the present, and it,\n",
      "Your love; never many Trigious inturburned night:\n",
      "Tell\n",
      "----------------------------------\n",
      "17436/46375 (epoch 46), train_loss = 1.441, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The reyer'd, that invel make last fitly flunes mine are the throw:\n",
      "Mocray to child, sir. If you charge for I fear, and, I know her less hurt from them.\n",
      "\n",
      "C\n",
      "----------------------------------\n",
      "17807/46375 (epoch 47), train_loss = 1.440, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The migger, that she illoothers and strairs to do it go't: musitors,\n",
      "My granted Froth?\n",
      "\n",
      "Second Gentleman:\n",
      "For modwifold the corked he thinks or no more go\n",
      "----------------------------------\n",
      "18178/46375 (epoch 48), train_loss = 1.439, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pardent to you\n",
      "His laugh. When my which I furnion.\n",
      "\n",
      "LUCIO:\n",
      "I dempt and secret cloy I let porat and lamping as my woman.\n",
      "\n",
      "BISHOP VI:\n",
      "Kind may thy noble\n",
      "----------------------------------\n",
      "18549/46375 (epoch 49), train_loss = 1.438, time/batch = 0.037\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The put and sense of this fill that ourself.\n",
      "\n",
      "PETRUCHIO:\n",
      "By hize degeth and folly: marque the lastest to make 'twake a brok?\n",
      "This conberch it will is a me\n",
      "----------------------------------\n",
      "18920/46375 (epoch 50), train_loss = 1.437, time/batch = 0.034\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The winge the fears we makes his saffent to--Narcalf thousand crown:\n",
      "What not we'erighted,\n",
      "Hull he most drownte?\n",
      "Stopound the king; for they get them must\n",
      "----------------------------------\n",
      "19291/46375 (epoch 51), train_loss = 1.436, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Duke of Englingly cold as hay these for thy royster,\n",
      "And I Adoeth and my turn; their most twres; asmes not perform by offence: which would sast your p\n",
      "----------------------------------\n",
      "19662/46375 (epoch 52), train_loss = 1.435, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The ribad frearfel.\n",
      "Dounded not reasy in the front hope half?\n",
      "The wones to procarridined, be grame good courrexy here to the gracious with tombrick thee?\n",
      "\n",
      "----------------------------------\n",
      "20033/46375 (epoch 53), train_loss = 1.434, time/batch = 0.041\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hath very workmant of the Duking: Protle Amal lock'd not?\n",
      "Then shall, for their highesire,\n",
      "If thou wilt thought loss tanus.\n",
      "\n",
      "ANTIGONUS:\n",
      "Was how it iss\n",
      "----------------------------------\n",
      "20404/46375 (epoch 54), train_loss = 1.433, time/batch = 0.042\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The king that now,\n",
      "For thou so such that not yours denied larketh, if flourt to you now is 'maging up the gentless the god maintrey:\n",
      "When Kate, in?\n",
      "\n",
      "GRUMI\n",
      "----------------------------------\n",
      "20775/46375 (epoch 55), train_loss = 1.432, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The call with repend and dead; that thousand God, speak him anot?\n",
      "\n",
      "FLORIZEL:\n",
      "It infury, thou gall, Warwick rathes of myself.\n",
      "\n",
      "STANLEY:\n",
      "O, thou speak 'bush\n",
      "----------------------------------\n",
      "21146/46375 (epoch 56), train_loss = 1.431, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The king, though's.\n",
      "\n",
      "TRANIO:\n",
      "Give them\n",
      "The onneher.\n",
      "\n",
      "KING HENRY VI?\n",
      "Fouls the heads,\n",
      "In time.\n",
      "Father, that heavening, as the shorten.\n",
      "Exceble!\n",
      "The morrow \n",
      "----------------------------------\n",
      "21517/46375 (epoch 57), train_loss = 1.431, time/batch = 0.036\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The secrive shall he hath fashied:\n",
      "Live me his father, as vengeas!\n",
      "Death?\n",
      "Sweek sap, and win too had rat-brankind too.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "\n",
      "All:\n",
      "Richard for\n",
      "----------------------------------\n",
      "21888/46375 (epoch 58), train_loss = 1.430, time/batch = 0.035\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The matter her eat's has thirging'\n",
      "\n",
      "PETRUCHIO:\n",
      "I'll say their cursed of pufficer'd me are before thy birce, my miselent:\n",
      "King me nor must hear the place!\n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        with tf.variable_scope(\"rnn\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print ('----------------------------------')\n",
    "            print ('SAMPLE GENERATED TEXT:')\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Thanks for completing this lesson!\n",
    "\n",
    "\n",
    "<h3>Authors:</h3>\n",
    "<article class=\"teacher\">\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone wp-image-2258 \" src=\"https://media.licdn.com/mpr/mpr/shrinknp_400_400/AAEAAQAAAAAAAAyFAAAAJGJlM2I2MmQzLTkxOWQtNDVhZi1hZGU0LWNlOWQzZDcyYjQ3ZA.jpg\" alt=\"Saeed Aghabozorgi\" width=\"178\" height=\"178\" /></div>\n",
    "<h4>Saeed Aghabozorgi</h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
